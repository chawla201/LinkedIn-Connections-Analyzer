{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login(browser):\n",
    "    username = input('Enter Username: ')\n",
    "    password = input('Enter Password: ')\n",
    "    browser.get('https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin')\n",
    "    time.sleep(3)\n",
    "    browser.find_element_by_name('session_key').send_keys(username + Keys.RETURN)\n",
    "    browser.find_element_by_name('session_password').send_keys(password + Keys.RETURN)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connections_scraper(browser):\n",
    "    connections_page = \"https://www.linkedin.com/search/results/people/?facetNetwork=%5B%22F%22%5D&origin=MEMBER_PROFILE_CANNED_SEARCH\"\n",
    "    browser.get(connections_page)\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    conn_num = soup.find_all('h3', class_='search-results__total')\n",
    "    num = int(conn_num[0].text.strip().split()[0])\n",
    "    time.sleep(3)\n",
    "    i = 2\n",
    "    x = 1\n",
    "    names = []\n",
    "    titles = []\n",
    "    locations = []\n",
    "    profiles = []\n",
    "    print('\\nScraping your connections...\\n')\n",
    "    while True:\n",
    "        browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(.75)\n",
    "        browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(.75)\n",
    "        browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(.75)\n",
    "        browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(.75)\n",
    "        browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "        soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "        name_tag = soup.find_all('span', class_='name actor-name')\n",
    "        title_tag = soup.find_all('p', class_='subline-level-1')\n",
    "        location_tag = soup.find_all('p', class_= 'subline-level-2')\n",
    "        profile_tag = soup.find_all('a', class_= 'search-result__result-link')\n",
    "        names += list(map(lambda x: x.text, name_tag))\n",
    "        titles += list(map(lambda x: x.text.replace('\\n','').strip(), title_tag))\n",
    "        locations += list(map(lambda x: x.text.replace('\\n','').strip(), location_tag))\n",
    "        profiles += list(map(lambda x: 'https://linkedin.com' + x['href'], profile_tag))[::2]\n",
    "        if len(names)>=num:\n",
    "            break\n",
    "        y = x\n",
    "        x = len(names)\n",
    "        if x==y:\n",
    "            break\n",
    "        browser.get('https://www.linkedin.com/search/results/people/?facetNetwork=%5B%22F%22%5D&origin=MEMBER_PROFILE_CANNED_SEARCH&page='+str(i))\n",
    "        i+=1\n",
    "        time.sleep(3)\n",
    "    df = pd.DataFrame({'Name':names, 'Title':titles, 'Location':locations, 'Profile':profiles})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(executable_path=\"E:/RnD/linkedin_scraper/chromedriver\", options= webdriver.ChromeOptions())\n",
    "login(browser)\n",
    "connections = connections_scraper(browser)\n",
    "browser.quit()\n",
    "connections.to_csv('connections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_scraper(df, browser):\n",
    "    num_projects = []\n",
    "    num_languages = []\n",
    "    top_skills = []\n",
    "    num_connections = []\n",
    "    positions = []\n",
    "    company = []\n",
    "    duration = []\n",
    "    institutes = []\n",
    "    courses = []\n",
    "    year_range = []\n",
    "    ex_profiles = []\n",
    "    ed_profiles = []\n",
    "    for profile in df['Profile']:\n",
    "        try:\n",
    "            browser.get(profile)\n",
    "            time.sleep(2)\n",
    "            browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(.75)\n",
    "            browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(.75)\n",
    "            browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(.75)\n",
    "            browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(.75)\n",
    "            browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(.75)\n",
    "            browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(.75)\n",
    "            browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(.75)\n",
    "            browser.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(.75)\n",
    "            soup = BeautifulSoup(browser.page_source, 'lxml')\n",
    "\n",
    "            conn_tag = soup.find_all('span', class_='t-16 t-bold')\n",
    "            if conn_tag[0].text.strip().split()[0].isdigit() or conn_tag[0].text.strip().split()[0] == '500+':\n",
    "                num_connections.append(conn_tag[0].text.strip().split()[0])\n",
    "            elif len(soup.find_all('span', class_='t-16 t-black t-normal'))>0:\n",
    "                num_connections.append(soup.find_all('span', class_='t-16 t-black t-normal')[0].text.strip().split()[0])\n",
    "            else:\n",
    "                num_connections.append(None)\n",
    "\n",
    "            accom_tag = soup.find_all('h3', class_='pv-accomplishments-block__count t-32 t-black t-normal pr3')\n",
    "            np = 0\n",
    "            nl = 2\n",
    "            for at in accom_tag:\n",
    "                if at.text.strip().split('\\n')[0].split()[-1] == 'projects' or at.text.strip().split('\\n')[0].split()[-1] == 'project':\n",
    "                    np = int(at.text.strip().split('\\n')[1])\n",
    "                if at.text.strip().split('\\n')[0].split()[-1] == 'languages' or at.text.strip().split('\\n')[0].split()[-1] == 'language':\n",
    "                    nl = int(at.text.strip().split('\\n')[1])\n",
    "            num_projects.append(np)\n",
    "            num_languages.append(nl)\n",
    "\n",
    "            skills_tag = soup.find_all('span', class_='pv-skill-category-entity__name-text')\n",
    "            ts = []\n",
    "            for st in skills_tag:\n",
    "                ts.append(st.text.strip())\n",
    "            top_skills.append(ts)\n",
    "\n",
    "            position_tag = soup.find_all('h3', class_='t-16 t-black t-bold')\n",
    "            ex_pos = list(map(lambda x: x.text.strip(), position_tag))\n",
    "            company_tag = soup.find_all('p', class_='pv-entity__secondary-title t-14 t-black t-normal')\n",
    "            ex_comp = list(map(lambda x: x.text.strip().split('\\n')[0], company_tag))\n",
    "            ex_duration_tag = soup.find_all('span', class_='pv-entity__bullet-item-v2')\n",
    "            durr = []\n",
    "            for dur in ex_duration_tag:\n",
    "                d_list = dur.text.strip().split()\n",
    "                if d_list[0].isdigit():\n",
    "                    if len(d_list)==2:\n",
    "                        if d_list[1] == 'mo' or d_list[1]=='mos':\n",
    "                            durr.append(int(d_list[0]))\n",
    "                        if d_list[1] == 'yr' or d_list[1]=='yrs':\n",
    "                            durr.append(int(d_list[0])*12)\n",
    "                    if len(d_list)==4:\n",
    "                        durr.append((int(d_list[0])*12)+int(d_list[2]))\n",
    "                else:\n",
    "                    durr.append(None)\n",
    "            x = min(len(ex_comp), len(ex_pos), len(durr))\n",
    "            ex_comp = ex_comp[:x]\n",
    "            ex_pos = ex_pos[:x]\n",
    "            durr = durr[:x]\n",
    "            ex_profiles += [profile]*x\n",
    "            positions += ex_pos\n",
    "            company += ex_comp\n",
    "            duration += durr\n",
    "\n",
    "            institute_tag = soup.find_all('h3', class_='pv-entity__school-name t-16 t-black t-bold')\n",
    "            inst = list(map(lambda x: x.text.strip(), institute_tag))\n",
    "            course_tag = soup.find_all('p', class_='pv-entity__secondary-title pv-entity__degree-name t-14 t-black t-normal')\n",
    "            course_t = list(map(lambda x: x.text.strip().split('\\n')[1], course_tag))\n",
    "            ed_date_tag = soup.find_all('p', class_='pv-entity__dates t-14 t-black--light t-normal')\n",
    "            ed_dates = list(map(lambda x: x.text.strip().split('\\n')[-1], ed_date_tag))\n",
    "            y = min(len(inst), len(course_t), len(ed_dates))\n",
    "            inst = inst[:y]\n",
    "            course_t = course_t[:y]\n",
    "            ed_dates = ed_dates[:y]\n",
    "            ed_profiles += [profile]*y\n",
    "            institutes += inst\n",
    "            courses += course_t\n",
    "            year_range += ed_dates\n",
    "        except:\n",
    "            continue\n",
    "    df['Number of connections'] = num_connections\n",
    "    df['Number of Projects'] = num_projects\n",
    "    df['Number of Languages known'] = num_languages\n",
    "    df['Top Skills'] = top_skills\n",
    "    exp_df = pd.DataFrame({'Profile':ex_profiles, 'Position':positions, 'Company':company, 'Duration':duration})\n",
    "    ed_df = pd.DataFrame({'Profile':ed_profiles, 'Institute':institutes, 'Degree':courses, 'Year range':year_range})\n",
    "    \n",
    "    return df, exp_df, ed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(executable_path=\"E:/RnD/linkedin_scraper/chromedriver\", options= webdriver.ChromeOptions())\n",
    "login(browser)\n",
    "data = pd.read_csv('connections.csv')\n",
    "df, exp, ed = profile_scraper(data, browser)\n",
    "browser.quit()\n",
    "df.to_csv('connections_data.csv', index=False)\n",
    "exp.to_csv('experience.csv', index=False)\n",
    "ed.to_csv('education.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(executable_path=\"E:/RnD/linkedin_scraper/chromedriver\", options= webdriver.ChromeOptions())\n",
    "login(browser)\n",
    "connections = connections_scraper(browser)\n",
    "conn, exp, ed = profile_scraper(connections, browser)\n",
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
